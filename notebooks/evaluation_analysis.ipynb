{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOP Q&A System Evaluation Analysis\n",
    "\n",
    "This notebook provides comprehensive evaluation and analysis of the SOP Q&A system using RAGAS metrics and performance benchmarks.\n",
    "\n",
    "## Contents\n",
    "1. Setup and Configuration\n",
    "2. Golden Dataset Analysis\n",
    "3. RAGAS Evaluation\n",
    "4. Performance Benchmarking\n",
    "5. Results Visualization\n",
    "6. Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "from sop_qa_tool.services.evaluation import EvaluationFramework\n",
    "from sop_qa_tool.services.rag_chain import RAGChain\n",
    "from sop_qa_tool.models.sop_models import GoldenDatasetItem\n",
    "from sop_qa_tool.config.settings import get_settings\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize settings and services\n",
    "settings = get_settings()\n",
    "print(f\"Running in {settings.mode} mode\")\n",
    "\n",
    "# Initialize RAG chain\n",
    "rag_chain = RAGChain()\n",
    "\n",
    "# Initialize evaluation framework\n",
    "eval_framework = EvaluationFramework(rag_chain)\n",
    "\n",
    "# Load golden dataset\n",
    "golden_dataset = eval_framework.load_golden_dataset()\n",
    "print(f\"Loaded {len(golden_dataset)} items from golden dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Golden Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze golden dataset composition\n",
    "df_golden = pd.DataFrame([item.dict() for item in golden_dataset])\n",
    "\n",
    "# Category distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Category distribution\n",
    "category_counts = df_golden['category'].value_counts()\n",
    "axes[0, 0].pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%')\n",
    "axes[0, 0].set_title('Question Categories')\n",
    "\n",
    "# Difficulty distribution\n",
    "difficulty_counts = df_golden['difficulty'].value_counts()\n",
    "axes[0, 1].bar(difficulty_counts.index, difficulty_counts.values)\n",
    "axes[0, 1].set_title('Question Difficulty Distribution')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "\n",
    "# Question length distribution\n",
    "question_lengths = df_golden['question'].str.len()\n",
    "axes[1, 0].hist(question_lengths, bins=10, alpha=0.7)\n",
    "axes[1, 0].set_title('Question Length Distribution')\n",
    "axes[1, 0].set_xlabel('Characters')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "\n",
    "# Expected answer length distribution\n",
    "answer_lengths = df_golden['expected_answer'].str.len()\n",
    "axes[1, 1].hist(answer_lengths, bins=10, alpha=0.7)\n",
    "axes[1, 1].set_title('Expected Answer Length Distribution')\n",
    "axes[1, 1].set_xlabel('Characters')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(f\"Total questions: {len(df_golden)}\")\n",
    "print(f\"Average question length: {question_lengths.mean():.1f} characters\")\n",
    "print(f\"Average answer length: {answer_lengths.mean():.1f} characters\")\n",
    "print(f\"Categories: {', '.join(category_counts.index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RAGAS Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run RAGAS evaluation\n",
    "print(\"Starting RAGAS evaluation...\")\n",
    "evaluation_result = await eval_framework.evaluate_rag_pipeline(golden_dataset)\n",
    "\n",
    "print(f\"\\nEvaluation completed in {evaluation_result.evaluation_time_seconds:.2f} seconds\")\n",
    "print(f\"Overall pass rate: {evaluation_result.overall_pass_rate:.2%}\")\n",
    "\n",
    "# Display metrics\n",
    "metrics_df = pd.DataFrame([\n",
    "    {\n",
    "        'Metric': metric_name,\n",
    "        'Score': metric_data['score'],\n",
    "        'Threshold': metric_data['threshold'],\n",
    "        'Passed': '✓' if metric_data['passed'] else '✗',\n",
    "        'Gap': metric_data['score'] - metric_data['threshold']\n",
    "    }\n",
    "    for metric_name, metric_data in evaluation_result.metrics.items()\n",
    "])\n",
    "\n",
    "print(\"\\nRAGAS Metrics Results:\")\n",
    "print(metrics_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize RAGAS metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Metrics vs Thresholds\n",
    "metrics_names = list(evaluation_result.metrics.keys())\n",
    "scores = [evaluation_result.metrics[m]['score'] for m in metrics_names]\n",
    "thresholds = [evaluation_result.metrics[m]['threshold'] for m in metrics_names]\n",
    "\n",
    "x = np.arange(len(metrics_names))\n",
    "width = 0.35\n",
    "\n",
    "axes[0, 0].bar(x - width/2, scores, width, label='Actual Score', alpha=0.8)\n",
    "axes[0, 0].bar(x + width/2, thresholds, width, label='Threshold', alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Metrics')\n",
    "axes[0, 0].set_ylabel('Score')\n",
    "axes[0, 0].set_title('RAGAS Metrics vs Thresholds')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(metrics_names, rotation=45)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Pass/Fail visualization\n",
    "passed_counts = sum(1 for m in evaluation_result.metrics.values() if m['passed'])\n",
    "failed_counts = len(evaluation_result.metrics) - passed_counts\n",
    "\n",
    "axes[0, 1].pie([passed_counts, failed_counts], \n",
    "               labels=['Passed', 'Failed'], \n",
    "               colors=['green', 'red'],\n",
    "               autopct='%1.1f%%')\n",
    "axes[0, 1].set_title('Metrics Pass/Fail Rate')\n",
    "\n",
    "# Score distribution\n",
    "axes[1, 0].hist(scores, bins=10, alpha=0.7, color='skyblue')\n",
    "axes[1, 0].axvline(np.mean(scores), color='red', linestyle='--', label=f'Mean: {np.mean(scores):.3f}')\n",
    "axes[1, 0].set_xlabel('Score')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].set_title('Score Distribution')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Gap analysis (score - threshold)\n",
    "gaps = [evaluation_result.metrics[m]['score'] - evaluation_result.metrics[m]['threshold'] \n",
    "        for m in metrics_names]\n",
    "colors = ['green' if gap >= 0 else 'red' for gap in gaps]\n",
    "\n",
    "axes[1, 1].bar(metrics_names, gaps, color=colors, alpha=0.7)\n",
    "axes[1, 1].axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "axes[1, 1].set_xlabel('Metrics')\n",
    "axes[1, 1].set_ylabel('Gap (Score - Threshold)')\n",
    "axes[1, 1].set_title('Performance Gap Analysis')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test queries for benchmarking\n",
    "test_queries = [item.question for item in golden_dataset[:5]]  # Use first 5 questions\n",
    "\n",
    "print(\"Starting performance benchmarking...\")\n",
    "benchmark_result = await eval_framework.benchmark_performance(\n",
    "    test_queries=test_queries,\n",
    "    concurrent_users=[1, 3, 5],\n",
    "    iterations=3\n",
    ")\n",
    "\n",
    "print(f\"Benchmark completed with {benchmark_result.test_queries_count} queries\")\n",
    "print(f\"Tested concurrent users: {benchmark_result.concurrent_users_tested}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Extract latency data\n",
    "users = benchmark_result.concurrent_users_tested\n",
    "latency_data = benchmark_result.results['latency_metrics']\n",
    "\n",
    "mean_latencies = [latency_data[u]['mean'] for u in users]\n",
    "p95_latencies = [latency_data[u]['p95'] for u in users]\n",
    "p99_latencies = [latency_data[u]['p99'] for u in users]\n",
    "\n",
    "# Latency vs Concurrent Users\n",
    "axes[0, 0].plot(users, mean_latencies, 'o-', label='Mean', linewidth=2)\n",
    "axes[0, 0].plot(users, p95_latencies, 's-', label='P95', linewidth=2)\n",
    "axes[0, 0].plot(users, p99_latencies, '^-', label='P99', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Concurrent Users')\n",
    "axes[0, 0].set_ylabel('Latency (seconds)')\n",
    "axes[0, 0].set_title('Response Latency vs Load')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Throughput vs Concurrent Users\n",
    "throughput_data = benchmark_result.results['throughput_metrics']\n",
    "throughputs = [throughput_data[u]['queries_per_second'] for u in users]\n",
    "\n",
    "axes[0, 1].bar(users, throughputs, alpha=0.7, color='orange')\n",
    "axes[0, 1].set_xlabel('Concurrent Users')\n",
    "axes[0, 1].set_ylabel('Queries per Second')\n",
    "axes[0, 1].set_title('System Throughput')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Latency distribution for highest load\n",
    "max_users = max(users)\n",
    "latency_stats = latency_data[max_users]\n",
    "latency_values = ['mean', 'p50', 'p95', 'p99']\n",
    "latency_scores = [latency_stats[v] for v in latency_values]\n",
    "\n",
    "axes[1, 0].bar(latency_values, latency_scores, alpha=0.7, color='lightcoral')\n",
    "axes[1, 0].set_ylabel('Latency (seconds)')\n",
    "axes[1, 0].set_title(f'Latency Distribution ({max_users} Users)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Performance summary table\n",
    "axes[1, 1].axis('tight')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "perf_summary = []\n",
    "for u in users:\n",
    "    perf_summary.append([\n",
    "        u,\n",
    "        f\"{latency_data[u]['mean']:.3f}s\",\n",
    "        f\"{latency_data[u]['p95']:.3f}s\",\n",
    "        f\"{throughput_data[u]['queries_per_second']:.2f}\"\n",
    "    ])\n",
    "\n",
    "table = axes[1, 1].table(cellText=perf_summary,\n",
    "                        colLabels=['Users', 'Mean Latency', 'P95 Latency', 'QPS'],\n",
    "                        cellLoc='center',\n",
    "                        loc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.5)\n",
    "axes[1, 1].set_title('Performance Summary')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results Analysis and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive analysis\n",
    "print(\"=== EVALUATION SUMMARY ===\")\n",
    "print(f\"Evaluation Date: {evaluation_result.timestamp.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Dataset Size: {evaluation_result.dataset_size} questions\")\n",
    "print(f\"Evaluation Time: {evaluation_result.evaluation_time_seconds:.2f} seconds\")\n",
    "print(f\"Overall Pass Rate: {evaluation_result.overall_pass_rate:.1%}\")\n",
    "\n",
    "print(\"\\n=== RAGAS METRICS ANALYSIS ===\")\n",
    "for metric_name, metric_data in evaluation_result.metrics.items():\n",
    "    status = \"PASS\" if metric_data['passed'] else \"FAIL\"\n",
    "    gap = metric_data['score'] - metric_data['threshold']\n",
    "    print(f\"{metric_name:20s}: {metric_data['score']:.3f} (threshold: {metric_data['threshold']:.3f}) [{status}] Gap: {gap:+.3f}\")\n",
    "\n",
    "print(\"\\n=== PERFORMANCE ANALYSIS ===\")\n",
    "target_latency_aws = 3.0  # seconds (from requirements)\n",
    "target_latency_local = 6.0  # seconds (from requirements)\n",
    "current_mode = settings.mode\n",
    "target_latency = target_latency_aws if current_mode == 'aws' else target_latency_local\n",
    "\n",
    "print(f\"Mode: {current_mode.upper()}\")\n",
    "print(f\"Target Latency: {target_latency}s\")\n",
    "\n",
    "for users in benchmark_result.concurrent_users_tested:\n",
    "    mean_lat = benchmark_result.results['latency_metrics'][users]['mean']\n",
    "    p95_lat = benchmark_result.results['latency_metrics'][users]['p95']\n",
    "    qps = benchmark_result.results['throughput_metrics'][users]['queries_per_second']\n",
    "    \n",
    "    lat_status = \"PASS\" if p95_lat <= target_latency else \"FAIL\"\n",
    "    print(f\"{users} users: Mean={mean_lat:.3f}s, P95={p95_lat:.3f}s [{lat_status}], QPS={qps:.2f}\")\n",
    "\n",
    "print(\"\\n=== KEY INSIGHTS ===\")\n",
    "\n",
    "# Identify strengths\n",
    "strong_metrics = [name for name, data in evaluation_result.metrics.items() \n",
    "                 if data['score'] - data['threshold'] > 0.05]\n",
    "if strong_metrics:\n",
    "    print(f\"✓ Strong performance in: {', '.join(strong_metrics)}\")\n",
    "\n",
    "# Identify areas for improvement\n",
    "weak_metrics = [name for name, data in evaluation_result.metrics.items() \n",
    "               if not data['passed']]\n",
    "if weak_metrics:\n",
    "    print(f\"⚠ Needs improvement: {', '.join(weak_metrics)}\")\n",
    "\n",
    "# Performance insights\n",
    "max_users_tested = max(benchmark_result.concurrent_users_tested)\n",
    "max_load_p95 = benchmark_result.results['latency_metrics'][max_users_tested]['p95']\n",
    "if max_load_p95 <= target_latency:\n",
    "    print(f\"✓ Performance target met even at {max_users_tested} concurrent users\")\n",
    "else:\n",
    "    print(f\"⚠ Performance degrades under load (P95: {max_load_p95:.3f}s > {target_latency}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations based on results\n",
    "recommendations = []\n",
    "\n",
    "# RAGAS-based recommendations\n",
    "for metric_name, metric_data in evaluation_result.metrics.items():\n",
    "    if not metric_data['passed']:\n",
    "        gap = metric_data['threshold'] - metric_data['score']\n",
    "        if metric_name == 'faithfulness':\n",
    "            recommendations.append(f\"Improve faithfulness (gap: {gap:.3f}) by enhancing citation accuracy and reducing hallucinations\")\n",
    "        elif metric_name == 'answer_relevancy':\n",
    "            recommendations.append(f\"Improve answer relevancy (gap: {gap:.3f}) by refining retrieval and context selection\")\n",
    "        elif metric_name == 'context_precision':\n",
    "            recommendations.append(f\"Improve context precision (gap: {gap:.3f}) by optimizing chunk size and retrieval parameters\")\n",
    "        elif metric_name == 'context_recall':\n",
    "            recommendations.append(f\"Improve context recall (gap: {gap:.3f}) by increasing retrieval diversity and top-k values\")\n",
    "\n",
    "# Performance-based recommendations\n",
    "high_latency_users = [u for u in benchmark_result.concurrent_users_tested \n",
    "                     if benchmark_result.results['latency_metrics'][u]['p95'] > target_latency]\n",
    "if high_latency_users:\n",
    "    recommendations.append(f\"Optimize performance for concurrent loads (fails at {min(high_latency_users)}+ users)\")\n",
    "\n",
    "# Dataset recommendations\n",
    "if len(golden_dataset) < 20:\n",
    "    recommendations.append(\"Expand golden dataset to at least 20-30 questions for more robust evaluation\")\n",
    "\n",
    "category_balance = df_golden['category'].value_counts()\n",
    "if category_balance.std() > category_balance.mean() * 0.5:\n",
    "    recommendations.append(\"Balance question categories in golden dataset for comprehensive evaluation\")\n",
    "\n",
    "print(\"=== RECOMMENDATIONS ===\")\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {rec}\")\n",
    "\n",
    "if not recommendations:\n",
    "    print(\"✓ System performance meets all targets. Consider expanding evaluation scope.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to files\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_dir = Path(\"../data/evaluation/results\")\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Export metrics summary\n",
    "metrics_df.to_csv(results_dir / f\"ragas_metrics_{timestamp}.csv\", index=False)\n",
    "\n",
    "# Export performance summary\n",
    "perf_df = pd.DataFrame(perf_summary, columns=['Users', 'Mean_Latency', 'P95_Latency', 'QPS'])\n",
    "perf_df.to_csv(results_dir / f\"performance_metrics_{timestamp}.csv\", index=False)\n",
    "\n",
    "# Export recommendations\n",
    "with open(results_dir / f\"recommendations_{timestamp}.txt\", 'w') as f:\n",
    "    f.write(\"SOP Q&A System Evaluation Recommendations\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        f.write(f\"{i}. {rec}\\n\")\n",
    "\n",
    "print(f\"Results exported to {results_dir}\")\n",
    "print(f\"Files created:\")\n",
    "print(f\"- ragas_metrics_{timestamp}.csv\")\n",
    "print(f\"- performance_metrics_{timestamp}.csv\")\n",
    "print(f\"- recommendations_{timestamp}.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}